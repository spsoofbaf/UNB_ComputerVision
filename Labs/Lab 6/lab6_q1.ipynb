{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1)\n",
        "a)\n",
        "It is defined as below:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MentbewSGBBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(10))"
      ],
      "metadata": {
        "id": "ezRyevG9Hnel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is the sequential model. Using the Conv2D function from the layers module and the supplied number of filters, kernel size, and activation function, convolutional layers are added.\n",
        "\n",
        "Using the MaxPooling2D function from the layers module and the provided pool size, the maximum pooling layers are added.\n",
        "\n",
        "The output of the last convolutional layer is flattened into a 1D array by the addition of the Flatten layer.\n",
        "\n",
        "Using the Dense function from the layers module and the supplied number of neurons and activation function, dense layers are added. Ten neurons make up the last dense layer, which is equivalent to the ten classes in the CIFAR-10 dataset.\n",
        "\n",
        "The loss function, optimizer, and evaluation metrics are then specified for the model by calling the compile method on the model. \n",
        "The model is then trained on the training set using the provided number of epochs and validation data by calling the fit method.\n"
      ],
      "metadata": {
        "id": "snGSwd-KHs0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)\n",
        "b)\n",
        "\n",
        "Using the provided code, if the inputs are four 50x50 RGB images and the batch size is four.A tensor of shape (4, 24, 24) would be produced by Conv2D(2, 3, strides=2, padding='valid')(x).\n",
        "The dimensions are broken down as follows:\n",
        "\n",
        "The first dimension is 4 (because of the batch size).\n",
        "\n",
        "The kernel size is 3x3 with two filters, and the input image is 50x50 with three channels. The output size will be (input_size - kernel_size + 1) / stride = (50 - 3 + 1) / 2 = 24 when padding='valid' and strides=2. The output tensor shape will therefore be (24, 24) for height and breadth, and 2 for the number of filters."
      ],
      "metadata": {
        "id": "BhKWalkiJD3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)\n",
        "c)\n",
        "\n",
        "The total number of parameters in the model can be computed as follows before two dense layers are added:\n",
        "\n",
        "With three input channels and 32 filters of size 3x3, the first convolutional layer has a total of (3*3*3 + 1) * 32 = 896 parameters.\n",
        "\n",
        "With 32 input channels (from the previous layer) and 64 3x3 filter elements, the second convolutional layer contains a total of (3*3*32 + 1) * 64 = 18496 parameters.\n",
        "\n",
        "With 64 filters of size 3x3 and 64 input channels (from the layer before), the third convolutional layer contains a total of (3*3*64 + 1) * 64 = 36928 parameters.\n",
        "\n",
        "The model has 56,320 parameters in total,"
      ],
      "metadata": {
        "id": "7BadgM_BJWnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)\n",
        "d)\n",
        "\n",
        "The output is flattened and then fed through two dense layers to conduct classification after the convolutional layers.\n",
        "\n",
        "64 neurons in the first dense layer each have a ReLU activation function, which accepts the preceding layer's flattened output as input. More intricate and nonlinear patterns in the data are learned using this layer.\n",
        "\n",
        "Ten neurons make up the second dense layer, which is equal to the number of classes in the CIFAR-10 dataset. The predicted class probabilities for each input image are produced using this layer as the model's final output. For image classification tasks, a common deep learning strategy is to combine dense layers with convolutional layers.\n",
        "\n",
        "The dense layers are in charge of learning to map the learned features to the associated class probabilities, while the convolutional layers are in charge of learning features from the input images."
      ],
      "metadata": {
        "id": "dqpiFIhDKM0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)\n",
        "e)\n",
        "\n",
        "Following the addition of the two dense layers, the total number of parameters in the model can be determined as follows:\n",
        "\n",
        "After flattening, the previous layer's 64 * 4 * 4 outputs and the first dense layer's 64 neurons add up to a total of (64 * 4 * 4) * 64 + 64 = 65600 parameters.\n",
        "\n",
        "With 10 neurons in the second dense layer and 64 outputs in the layer before it, a total of 64 * 10 + 10 = 650 parameters are present.\n",
        "The total number of parameters in the two dense layers after adding together all of the parameters is 66250 (65600 + 650).\n",
        "\n",
        "Thus, after combining the two dense layers, the total number of parameters in the model is 56,320 + 66,250 = 122,570."
      ],
      "metadata": {
        "id": "R9HU9ykoKl0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)\n",
        "f)\n",
        "\n",
        "Depending on the context of the problem and the dataset being utilized, the test accuracy of 70% attained by the CNN in the example may be regarded good.\n",
        "\n",
        "Different strategies, such as boosting model complexity, applying transfer learning, modifying hyperparameters, enhancing data, or employing an ensemble of models, can be utilized to boost accuracy.\n",
        "\n",
        "Each of these strategies should be investigated in the context of the particular issue being addressed because they each have the potential to improve performance.\n",
        "\n",
        "For instance: The learning rate, batch size, and optimizer, among other hyperparameters, can significantly affect the model's accuracy.\n",
        "\n",
        "Performance can be enhanced by experimenting with various hyperparameters and tweaking them for the particular task."
      ],
      "metadata": {
        "id": "0qhRvdElLGXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IKZFienGLkzu"
      }
    }
  ]
}